{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15baae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "import os\n",
    "\n",
    "xml_path = './MAGIC-Lab/images_4asec_2487_addMarkers.xml'\n",
    "\n",
    "with open(xml_path) as fd:\n",
    "    doc = xmltodict.parse(fd.read())\n",
    "camera = doc['document']['chunk']['cameras']['camera']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a981608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from kornia.geometry import conversions\n",
    "from kornia.geometry.conversions import QuaternionCoeffOrder as Order\n",
    "\n",
    "images_path = './MAGIC-Lab/images_4asec'\n",
    "for img_info in camera:\n",
    "    path_name = img_info['@label']\n",
    "    pose_path = f'{images_path}/{path_name}.txt'\n",
    "    #To avoid key error, using built in function \".get(key, default)\".\n",
    "    if(img_info.get('@enabled', False) and img_info['@enabled'] == 'false'):\n",
    "        continue\n",
    "    pose_matrix = img_info['transform']\n",
    "    pose_matrix = pose_matrix.split()\n",
    "\n",
    "    with open(f'{pose_path}', 'w') as f:\n",
    "        for i, el in enumerate(pose_matrix, start = 1):\n",
    "            if i%4 != 0: \n",
    "                f.write(el + \"\\t\")\n",
    "            else:\n",
    "                f.write(el + \"\\n\")\n",
    "                \n",
    "    T = np.loadtxt(pose_path, dtype='float32') # homogeneous transformation matrix, camera to world\n",
    "    R = torch.tensor(T[:3, :3]) # rotation matrix\n",
    "    r = torch.tensor(T[:3, 3]) # translation vector\n",
    "    q = conversions.rotation_matrix_to_quaternion(R, order=Order.WXYZ) # quaternion\n",
    "    if q[0] < 0:\n",
    "        q = -q # constrain q0 > 0\n",
    "    pose_7params = torch.hstack((r, q))\n",
    "    pose_7params = pose_7params.numpy()\n",
    "    \n",
    "    with open(f'{images_path}/{path_name}_7Params.txt', 'w') as f:\n",
    "        for el in pose_7params:\n",
    "            f.write(str(el) + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81b59404",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "images_path = './MAGIC-Lab/images_4asec'\n",
    "dir_list = os.listdir(images_path)\n",
    "training_nums = 2000\n",
    "\n",
    "id_list = []\n",
    "for file in dir_list:\n",
    "    if file.split('.')[1] == 'txt':\n",
    "        id_list.append(file.split('.')[0])\n",
    "\n",
    "training_list = np.random.choice(id_list, training_nums, replace = False)\n",
    "testing_list = list(set(id_list).difference(set(training_list)))\n",
    "\n",
    "datasets_path = './MAGIC-Lab/datasets'\n",
    "if not os.path.isdir(datasets_path):\n",
    "    os.mkdir(datasets_path)\n",
    "if not os.path.isdir(f'{datasets_path}/training_set'):\n",
    "    os.mkdir(f'{datasets_path}/training_set')\n",
    "if not os.path.isdir(f'{datasets_path}/testing_set'):\n",
    "    os.mkdir(f'{datasets_path}/testing_set')\n",
    "\n",
    "training_path = f'{datasets_path}/training_set/'\n",
    "testing_path = f'{datasets_path}/testing_set/'\n",
    "for file in dir_list:\n",
    "    if file.split('.')[0] in training_list:\n",
    "        shutil.copy(os.path.join(images_path, file), os.path.join(training_path, file))\n",
    "    if file.split('.')[0] in testing_list:\n",
    "        shutil.copy(os.path.join(images_path, file), os.path.join(testing_path, file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6147b6e",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4186bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from kornia.geometry import conversions\n",
    "from kornia.geometry.conversions import QuaternionCoeffOrder as Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24598a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabDataset(Dataset):\n",
    "    \"\"\"Lab Dataset.\n",
    "\n",
    "    Attributes:\n",
    "        imgs (torch.Tensor): Images.\n",
    "        poses (torch.Tensor): Camera poses.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, transform, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path (str): Path of Lab dataset.\n",
    "            train (bool): If True, return training data; else, return testing data.\n",
    "            transform (torchvision.transforms): Image transform.\n",
    "        \"\"\"\n",
    "        dir_path = path\n",
    "\n",
    "        if train:\n",
    "            split_path = os.path.join(dir_path, 'training_set')\n",
    "        else:\n",
    "            split_path = os.path.join(dir_path, 'testing_set')\n",
    "\n",
    "        total_frames = int(len(os.listdir(split_path))/2)\n",
    "        all_list = os.listdir(split_path)\n",
    "        img_list = []\n",
    "        for file in all_list:\n",
    "            if(file.split('.')[-1] != \"txt\"):\n",
    "                img_list.append(file + \"\")\n",
    "                \n",
    "        first_img_dir = img_list[0]\n",
    "        img_temp = Image.open(os.path.join(split_path, first_img_dir)) \n",
    "        print(img_temp)\n",
    "        img_temp = transform(img_temp)\n",
    "        print(img_temp)\n",
    "        height, width = img_temp.shape[-2:]\n",
    "        self.imgs = torch.empty(total_frames, 3, height, width)\n",
    "        self.poses = torch.empty(total_frames, 7)\n",
    "\n",
    "        index = 0\n",
    "        for file in img_list:\n",
    "            img_path = os.path.join(split_path, file)\n",
    "            img = Image.open(img_path)\n",
    "            img = transform(img)\n",
    "            self.imgs[index] = img\n",
    "            pose_txt_path = os.path.join(split_path, f\"{file.split('.')[0]}.txt\")\n",
    "            T = np.loadtxt(pose_txt_path, dtype='float32') # homogeneous transformation matrix, camera to world\n",
    "            R = torch.tensor(T[:3, :3]) # rotation matrix\n",
    "            r = torch.tensor(T[:3, 3]) # translation vector\n",
    "            q = conversions.rotation_matrix_to_quaternion(R, order=Order.WXYZ) # quaternion\n",
    "            if q[0] < 0:\n",
    "                q = -q # constrain q0 > 0\n",
    "            pose = torch.hstack((r, q))\n",
    "            self.poses[index] = pose\n",
    "            index += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.imgs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Input index.\n",
    "\n",
    "        Returns:\n",
    "            {'image' (torch.Tensor): Image,\n",
    "             'pose' (torch.Tensor): Pose of the image.}\n",
    "\n",
    "        Shape:\n",
    "            {'image': (3, height, width),\n",
    "             'pose': (7).}\n",
    "        \"\"\"\n",
    "        return {'image': self.imgs[idx],\n",
    "                'pose': self.poses[idx]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76c1c524",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'lab_data/datasets'\n",
    "resize = 1080\n",
    "batch_size = 16\n",
    "num_workers = 0\n",
    "pin_memory = True\n",
    "lr = 0.001\n",
    "mean = [0.4702,0.4557,0.4363] #calculated by get_datas_mean_std function\n",
    "std = [0.2696,0.2698,0.2564]\n",
    "epochs = 300\n",
    "checkpoint_epochs = 10\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=(resize,resize)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9af497c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1920x1080 at 0x228AB2D2B00>\n",
      "tensor([[[ 0.9178,  0.9178,  0.9178,  ...,  1.3106,  1.3106,  1.3106],\n",
      "         [ 0.9178,  0.9178,  0.9178,  ...,  1.3106,  1.3106,  1.3106],\n",
      "         [ 0.9178,  0.9178,  0.9178,  ...,  1.3106,  1.3106,  1.3106],\n",
      "         ...,\n",
      "         [-1.1913, -0.8131, -0.2167,  ...,  0.0887,  0.0887,  0.1178],\n",
      "         [-1.2059, -0.8713, -0.3040,  ...,  0.1178,  0.1178,  0.1324],\n",
      "         [-1.2059, -0.9150, -0.3622,  ...,  0.1324,  0.1324,  0.1469]],\n",
      "\n",
      "        [[ 0.9709,  0.9709,  0.9709,  ...,  1.3197,  1.3197,  1.3197],\n",
      "         [ 0.9709,  0.9709,  0.9709,  ...,  1.3197,  1.3197,  1.3197],\n",
      "         [ 0.9709,  0.9709,  0.9709,  ...,  1.3197,  1.3197,  1.3197],\n",
      "         ...,\n",
      "         [-1.1076, -0.7297, -0.1338,  ...,  0.1569,  0.1569,  0.1860],\n",
      "         [-1.1222, -0.7879, -0.2210,  ...,  0.1860,  0.1860,  0.2005],\n",
      "         [-1.1222, -0.8315, -0.2791,  ...,  0.2005,  0.2005,  0.2151]],\n",
      "\n",
      "        [[ 1.0973,  1.0973,  1.0973,  ...,  1.3573,  1.3573,  1.3573],\n",
      "         [ 1.0973,  1.0973,  1.0973,  ...,  1.3573,  1.3573,  1.3573],\n",
      "         [ 1.0973,  1.0973,  1.0973,  ...,  1.3573,  1.3573,  1.3573],\n",
      "         ...,\n",
      "         [-1.1051, -0.7075, -0.0804,  ...,  0.0267,  0.0267,  0.0573],\n",
      "         [-1.1204, -0.7687, -0.1722,  ...,  0.0573,  0.0573,  0.0726],\n",
      "         [-1.1204, -0.8145, -0.2333,  ...,  0.0726,  0.0726,  0.0878]]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "trainset = LabDataset(path, transform=transform, train=True)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=num_workers, pin_memory=pin_memory,\n",
    "                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e5fd13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d43b054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch200Python3",
   "language": "python",
   "name": "kernel_torch200"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
